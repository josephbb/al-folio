<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://joebakcoleman.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://joebakcoleman.com/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-11-30T05:16:15+00:00</updated><id>https://joebakcoleman.com/feed.xml</id><title type="html">blank</title><subtitle>Collective behavior from fish to fascists.
</subtitle><entry><title type="html">Replication failures from fishy science</title><link href="https://joebakcoleman.com/blog/2023/replication/" rel="alternate" type="text/html" title="Replication failures from fishy science" /><published>2023-11-29T00:00:00+00:00</published><updated>2023-11-29T00:00:00+00:00</updated><id>https://joebakcoleman.com/blog/2023/replication</id><content type="html" xml:base="https://joebakcoleman.com/blog/2023/replication/"><![CDATA[<h2 id="a-fishy-start-to-my-career">A fishy start to my career</h2>

<p>Although I now work on <em>Homo sapiens</em>, I got my start in the world of science studying the humble blind cavefish, <em>Astyanax mexicanus</em>. These are fascinating creatures that have evolved to live in absolute darkness and feast on bat guano. It’s a rare corner of science where you can use the word troglodytic without any negative connotations.</p>

<p>My undergraduate and Master’s advisor, Sheryl Coombs, was particularly interested in how fish orient to currents, a behavior called rheotaxis. For most species at most stages in their lives, if you plunk them in moving water, they face upstream. We wanted understand the multi-sensory basis of this behavior, because although it is a simple one-dimensional output it can rely on input from almost any sensory system: vision, vestibular cues, their flow-sensing lateral line system, smell, touch… you name it.</p>

<p><img src="/assets/img/Astyanax_mexicanus_01.jpg" alt="Blind cavefish" width="750" /></p>

<h2 id="swimming-upstream">Swimming upstream</h2>

<p>The literature on the subject stretched back about 100 years, with a paper every 5-40 years trying something new and declaring that fish do or do not use a given sensory system. Lyon in 1904 put fish in a dark tank with running water, cracked the lid, and concluded they needed vision to orient to currents. Hofer in 1908 declared they didn’t need vision, but that they needed the lateral line system instead. Dijkgraaf repeated Lyon’s experiments in 1963 and declared him correct. It flipped and flopped a few more times after that. The literature was a mess.</p>

<p>Of course, the elephant in the room was that our friends, the blind cavefish, can orient to currents just fine absent eyes. Vision is clearly not strictly required. Enter John Montgomery in what the Zoomers refer to as the late 20<sup>th</sup> century (1997). He ran very careful experiments with new techniques to answer the question once and for all (Montgomery et al. 1997). He knocked out the lateral line systems of three species of fish—including blind cave fish—and found that at low flow speeds, the lateral line was necessary, but not at high flow speeds. The replications and failed replications throughout the 20<sup>th</sup> century were explained by what psychologists these days are calling a hidden moderator: flow speed.</p>

<p>The results couldn’t be more obvious and I’ve included them below. The circles are with the lateral line fully knocked out and the squares are with the lateral line intact. The open and closed circles are interesting here as well. Without getting too into the weeds, the circles indicate scraping off the lateral line system’s cells or treating the fish with cobalt chloride. The squares indicate using a specific antibiotic to selectively knock out part of the lateral line system and the control. Solving  a long-standing debate, this work was rightly published in <em>Nature</em>. A rarity for the field.</p>

<p><img src="/assets/img/montgomery1997.png" alt="Blind cavefish" width="750" /></p>

<h2 id="a-failed-replication">A failed replication.</h2>
<p>One of the really cool things about blind cave fish is that they’re the same species as a sighted fish called a Mexican Tetra, which looks entirely different (it has eyes). If you breed two blind cave fish from different caves, they generally produce sighted offspring. For one of the first experiments I designed, more than a decade ago, I wanted to redo the Montgomery et al. work, comparing sighted and blind fish. We thought that maybe the blind fish would be a bit more sensitive, as they had more elaborate lateral lines and no eyes to take up valuable neural real estate. Going downstream is also terrible for them. Leaving the cave during a flood means certain death.</p>

<p>I needed to knock out the lateral line. The prevailing wisdom at the time was that Cobalt Chloride could be unethical to use, as it is more globally toxic. Gentamycin supposedly only knocked out part of the lateral line, but its cousins neomycin and streptomycin took out the whole thing. It’s reversible too, so two weeks later the fish are fine and back to normal. You can also verify the lateral line is knocked out with a harmless fluorescent dye called DASPEI.</p>

<p>I learned all of these techniques, knock their lateral lines out, and the damndest thing happens (Bak-Coleman et al. 2013). The fish are pointed upstream just fine at speeds of 3 and 7 cm/s. These are well below what was required in the Montgomery work. Same species, and this is absent both a lateral line and visual cues. It didn’t really matter how you measured it or what stats you ran. This is a godsend because I knew so little about stats at the time and even used dynamite plots.</p>

<p><img src="/assets/img/RI2.png" alt="Blind cavefish" width="750" /></p>

<h2 id="something-fishy-is-going-on">Something fishy is going on</h2>

<p>Montgomery in 1997 had cleverly replicated the effect across three very different species. One was a saltwater fish that lived in Antarctica, the second a species endemic to New Zealand that lived in fast-moving freshwater streams. The third, our blind cave fish, found in mostly still, pitch-black water. We have the gamut of temperature gradients, flow gradients, across saltwater and freshwater. The effects are consistently huge, as were ours across the sighted and blind morphs.</p>

<p>We couldn’t write this one off to heterogeneity or random noise. QRPs, which were not formally defined at the time, might jitter a p-value below .05, but they aren’t going to generate consistent psychometric functions across multiple species of fish and conditions. Fraud was likewise not a satisfactory explanation, as we knew the authors of the original study well, and they were fastidious with data preservation and more than happy to mail us their video cassettes (we declined, shipping from Australia was beyond our budget). It was clear nothing metaphorically fishy was going on, but equally clear something literally fishy was going on. We found ourselves back in the trap of Lyon, Hofer, and Dijkgraaf, trying to reconcile obvious but conflicting results.</p>

<h2 id="teasing-it-out">Teasing it out</h2>

<p>Sheryl, being the epitome of a good scientist, put the whole lab to work reading the literature to figure out what was going on. We went through every study on the orientation of fish to currents, examining all the details of the methods. How fast was the water? How did they create the flow? Turbulent? What species? What are the characteristics of that species? What did they use to knock out the lateral line? How old were the fish?</p>

<p>Something jumped out at us. If you physically damage the neuromasts via scraping or something very cold, you tend to knock out rheotaxis. The same was true of using cobalt chloride, which was found post-Montgomery to be toxic in fish at the doses used. Other studies with similar results had replicated these methods. We hadn’t used them out of a duty of care to the fish. Maybe the fish weren’t just unable to orient to currents, they were poisoned or physically harmed.</p>

<p>It’s a nice story that neatly explains the discrepancies, except there’s a confound. The lateral line system is divided into two separate systems, one that is on the surface of the fish and the other that is deeply buried in canals. They function differently, with the superficial system detecting low-frequency flow and the canal system detecting high-frequency flow. An alternative explanation is that the superficial neuromasts are important, the canals aren’t. Gentamycin was believed to be selective for canals, so we can’t tease apart this possibility from argument alone. We also weren’t really in the mood to start laboriously scraping and poisoning fish if we didn’t have to.</p>

<p>We started digging into the various aminoglycoside antibiotics and found that some are much better than others at reliably knocking out the lateral line at standard doses (Kulpa et al. 2015). For a host of reasons, this can possibly explain the apparent selective blocking of the lateral line system by Gentamycin. It can explain why gentamycin didn’t stop rheotaxis in Montgomery, but doesn’t get us any closer to figuring out why we couldn’t stop rheotaxis using the good stuff. It did, however, point us in the right direction for making sure we knocked out the lateral line—superficial and canal.</p>

<p><img src="/assets/img/Kulpa.png" alt="Blind cavefish" width="750" /></p>

<p>With this in hand, we started looking at the other differences. Another thing stuck out. Setting aside the Montgomery results on blind cave fish, rheotaxis seemed to depend on the lateral line for fish that spend their time on the substrate (Bak-Coleman Et al. 2014). Sure enough, when we went to test in a couple of species the results were clear. The two non-cavefish species Montgomery tested, despite their differences, were bottom-dwelling fish. The pattern was fairly consistent across the literature. We also pulled at other threads, the role of turbulence and and developmental stage. The list goes on, but we conducted a ton of experiments in a pretty short 2-3 year time span. We left one, in my memory, in the file-drawer which will be a story for the next blog post.</p>

<p>The story we put together was a complicated one but mechanistically made sense. Fish that are coupled to the substrate need the lateral line, because that coupling deprives them of vestibular cues. If fish have vision, they use it. How you knock out the lateral line matters, both in terms of toxicity and selectivity but also because some of the drugs just aren’t very reliable. Recently, I teamed back up with my Master’s advisor and John Montgomery of <em>Nature</em> 1997 fame to write a review article summarizing this and other work (Coombs, Montgomery and Bak-Coleman 2020). Give it a read if you want to hear more of the story.</p>

<h2 id="fish-buckets-uphill-both-ways">Fish buckets, uphill both ways.</h2>

<p>It’s been a while since I’ve done work on fish, and I’ve been almost exclusively working on humans for at least the last 6-7 years. Anyone conducting research into human behavior invariably runs into discourse around failed replications, what they mean, how they should be interpreted, and how to prevent them. These discussions often extend to the whole of science, suggesting how all fields should conduct their work, engage in replication, interpret replications, and so forth.</p>

<p>Whenever these discussions come up, I find myself at times at odds with some of the prevailing thinking. Tonight, it occurred to me that some portion of this disconnect goes back to my early work in the area. My first attempt at science was failing to replicate a study in <em>Nature</em>, but it felt exactly nothing like failing to replicate a study in <em>Nature</em> on humans might feel. It didn’t erode my faith in the original results or raise any concerns about the researchers. It was a mystery to solve, and the authors of the original paper felt much the same way.</p>

<p>There were parsimonious explanations that could have been cited to explain the replication failure, more than enough theoretical grounds to reject the old finding. But rejecting that finding was not the goal. Rather, the goal was to develop a cohesive understanding of all of the data in their totality. There were no concerns about fiddling with p-values because the effects were visible in the rawest of data—video tapes of the fish swimming. Nor were there worries about outcomes—it’s hard to mess up measuring a fish swimming upstream vs. not. We didn’t have pre-registration in any formal sense, but both labs had protocols before running every set of experiments, detailing what we were doing and why. QRPs hadn’t been defined but they wouldn’t have done us any good, because we weren’t hanging our hats or theory on the p-values from a single statistical test. Our file drawers were tiny, and limited to a single study we realized was hopelessly confounded.</p>

<p>I’m reading what I wrote in the last paragraph and recognizing that it sounds a lot like an old man waxing poetic about the simpler times before scientific reform, the replication crisis, and this QRP-hacking-fraud hooey. When we’d carry fish uphill both ways (four floors up, four down) just to look at the pure, unadulterated data without those fiddly statistical tests. When replication was just part of an honest day’s work, etc., etc…</p>

<p>This is most assuredly not the point of this post. I don’t think that research was better, and there’s plenty I’d do differently—particularly in the analysis. Nor do I think it was made possible because the standards were more lax then. Animal behavior hasn’t come that far, and I’m pretty sure we could conduct and publish the exact same experiments today. Probably with the same stats. It also isn’t the case that animal behavior is free from problems, as anyone following the <a href="https://www.science.org/content/article/university-investigation-found-prominent-spider-biologist-fabricated-falsified-data">spider literature in early 2020 is well-aware</a>.</p>

<p>While I’m proud of that work, I’d stop short of saying I’m 100% certain we got it right. But I’m sure we made progress. There could be a masters student in a lab trying and failing to replicate our research. Unlike what one might expect, the thought is absolutely <em>thrilling</em>, and would probably be the coolest email I could receive short of a tenure-track offer. I would love to find out that I’d gotten something wrong, that the story is even richer or simpler than I’d supposed. Especially if I’m not the one carrying the bucket.</p>

<h2 id="deforming-science">Deforming Science</h2>

<p>In reflecting on the few corners of science I’ve experienced, I can’t help but wonder what else is out there. The fish story is one of failures to replicate, superficially resembling our discourse in social science. Yet, the similarities between fish and the replication crisis as we understand it today start, and end, at the word replication. In fish orienting to currents, the “replication failures” dated back to not only before the replication crisis—but only four short years after Pearson’s p-values and decades before Fisherian p-values and Neyman-Pearson Null Hypothesis Significance Testing. There was no p-value to p-hack when this century-long train of failed replications started.</p>

<p>Obviously, the same is not true across science, and some findings are entirely propped up on escaping from the file-drawer, finding the right path in a forking garden, or outright fraud. Some fields really need accountability more than anything to make sure they don’t engage in these temptations. Still, others might not have any issues with replication but spend considerably more time just cataloging things as fodder for other research. Or they might have clear theory that leads to obvious effects which would replicate if true, but face challenges of how to physically create a mechanism for collecting the requisite data. Some disciplines might not have a snowflake’s chance in hell of ever replicating anything, because the opportunity, ethical or practical costs of replication may not be worth it. I’m sure I’m missing a host of other possibilities.</p>

<p>The point is that when we talk about replication crises, scientific reform, open science, or how science “should be,” I suspect the biggest point of contention is that very few of us have identical ideas of what science even is. We all carry some history of the research we’ve done, the methods we use, the questions we ask, and the people we’ve looked up to. For me, that history has led to skepticism that science is well enough defined to be capable of reform. I struggle to understand how we could give ESP replication failures and rheotactic replication failures the same diagnosis and prescription. Maybe the answer instead is to deform science, recognizing it for the amorphous blob that it is and trying to figure out how we can use it to sort out what the hell is going on.</p>

<h2 id="references">References</h2>

<ol>
  <li>
    <div class="csl-entry">Lyon, E. P. (1904). On rheotropism. I.—Rheotropism in fishes. <i>American Journal of Physiology--Legacy Content</i>. http://onlinelibrary.wiley.com/doi/10.1111/j.1469-185X.1974.tb01173.x/full</div>
  </li>
  <li>
    <div class="csl-entry">Dijkgraaf, S. (1963). The functioning and significance of the lateral-line organs. <i>Biological Reviews of the Cambridge Philosophical Society</i>, <i>38</i>, 51–105. https://doi.org/10.1111/j.1469-185X.1963.tb00654.x</div>
  </li>
  <li>
    <div class="csl-entry">Montgomery, J. C., Baker, C. F., &#38; Carton, A. G. (1997). <i>Montgomery et al. 1997 rheotaxis</i>. <i>389</i>(October), 960–963.</div>
  </li>
  <li>
    <div class="csl-entry">Bak-Coleman, J., Court, A., Paley, D. A., &#38; Coombs, S. (2013). The spatiotemporal dynamics of rheotactic behavior depends on flow speed and available sensory information. <i>The Journal of Experimental Biology</i>, <i>216</i>(Pt 21), 4011–4024. https://doi.org/10.1242/jeb.090480</div>
  </li>
  <li>
    <div class="csl-entry">Kulpa, M., Bak-Coleman, J., &#38; Coombs, S. (2015). The lateral line is necessary for blind cavefish rheotaxis in non-uniform flow. <i>Journal of Experimental Biology</i>, <i>218</i>(10), 1603–1612. https://doi.org/10.1242/jeb.119537</div>
  </li>
  <li>
    <div class="csl-entry">Bak-Coleman, J., &#38; Coombs, S. (2014). Sedentary behavior as a factor in determining lateral line contributions to rheotaxis. <i>Journal of Experimental Biology</i>, <i>217</i>(13), 2338–2347. https://doi.org/10.1242/jeb.102574</div>
  </li>
  <li>
    <div class="csl-entry">Coombs, S., Bak-Coleman, J., &#38; Montgomery, J. (2020). Rheotaxis revisited: a multi-behavioral and multisensory perspective on how fish orient to flow. In <i>The Journal of experimental biology</i> (Vol. 223, Issue 23). NLM (Medline). https://doi.org/10.1242/jeb.223008</div>
  </li>
</ol>]]></content><author><name></name></author><category term="metascience" /><category term="statistics" /><category term="fish" /><category term="replication" /><category term="fish" /><category term="rheotaxis" /><summary type="html"><![CDATA[The title is not what it seems.]]></summary></entry><entry><title type="html">Rigor Desiderada</title><link href="https://joebakcoleman.com/blog/2023/Harvard/" rel="alternate" type="text/html" title="Rigor Desiderada" /><published>2023-06-20T05:12:00+00:00</published><updated>2023-06-20T05:12:00+00:00</updated><id>https://joebakcoleman.com/blog/2023/Harvard</id><content type="html" xml:base="https://joebakcoleman.com/blog/2023/Harvard/"><![CDATA[<h1 id="yada-yada-falsificada">Yada, Yada, Falsificada</h1>

<p>On June 16<sup>th</sup>, Stephanie Lee released <a href="https://www.chronicle.com/article/a-weird-research-misconduct-scandal-about-dishonesty-just-got-weirder">an article</a> in the <em>The Chronicle</em> detailing a new chapter in a long-running scandal over data mishandling in a now-retracted <a href="https://www.pnas.org/doi/10.1073/pnas.1209746109">paper</a> about honesty. We’ve come to learn that a second researcher is being investigated by Harvard, and has currently been <a href="https://www.hbs.edu/faculty/Pages/profile.aspx?facId=271812">placed on leave</a>. While the full report has not been released (and may never be), it looks a lot like Harvard found something pretty damning. If none of this sounds familiar, I suggest reading her article before going much further.</p>

<p>Shortly after Stephanie’s article, <a href="https://datacolada.org/">Data Colada</a> announced a four-part blog series entitled “Data Falsficada” examining fraud on the part of Harvard Business School Professor Francesca Gino. This blog series is ostensibly a teaser of a longer report, compiled starting in 2021 by the Blog’s authors and other “anonymous researchers.”</p>

<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">The first of a four-post series on Data Falsificada<a href="https://t.co/hmKOxlsqBX">https://t.co/hmKOxlsqBX</a></p>&mdash; Data Colada (@DataColada) <a href="https://twitter.com/DataColada/status/1670073988201029632?ref_src=twsrc%5Etfw">June 17, 2023</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<h2 id="blog-post-1-oh-sheet">Blog post 1: Oh sheet.</h2>

<p>The <a href="https://datacolada.org/109">first blog post</a> examines <a href="https://www.pnas.org/doi/full/10.1073/pnas.1209746109">already-retracted</a> study, which claimed to show that signing at the top of a document leads to greater dishonesty than signing at the bottom. The key finding can be found in this <em>very chic</em> drop-shadow dynamite plot:</p>

<p><img src="/assets/img/pnas1.png" alt="alt text" title="Title" width="750" />
<em>Figure 1 from original manuscript. Caption: “Reported and actual number of math puzzles solved by condition, experiment 1 (n = 101). Error bars represent SEM.”</em></p>

<p>Data Colada’s blog post provides compelling evidence that rows were moved around inside of an excel file. The trick here is that excel files are kinda neat, and don’t both re-organizing the deeper data every time you move something around. Instead, they store a record of what went where. Digging into this record shows that a few of the observations appear to be out of order or duplicated.</p>

<h2 id="fill-em-all-and-let-god-sort-em-out">Fill ‘em all and let god sort ‘em out</h2>

<p>Looking at the swapped around datapoints, a pattern emerges. They’re very neatly aligned with the hypothesized effect, and <em>located on the extrema of each distribution</em>. This second point is absolutely critical. The authors argue that even if the data were inadvertently mishandled, the chance that <em>these</em> observations would have been mishandled and none other is extremely low. They run a statistical test (more on that later) to generate a tiny p-value as quantitative evidence that this pattern could not possibly be expected by chance.</p>

<p>It’s not explicitly stated here, but the null model is that the altered rows are drawn at random from all of the possible rows. To start scratching at whether this is plausible, we need to construct something that looks like the dataset not <em>as is</em> but <em>as was</em>. We can do that with a few lines of python, borrowing from the bottom of their blog post’s estimate of what the conditions likely were.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">7</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">12</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">51</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">52</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">101</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">study_one</span><span class="p">.</span><span class="nf">drop_duplicates</span><span class="p">()</span>
</code></pre></div></div>

<p>Let’s sort these by condition, but lets <em>hypothetically</em> assume that conditions had text labels in a column at an earlier stage—Control, Top, Bottom. Something mildly interesting happens. All of the out-of-sorts rows are next to one another,</p>

<p><img src="/assets/img/sortayada.png" alt="alt text" title="Caption" width="750" /></p>

<p><em>Rows sorted by “Name”, with a secondary sort on the original row numbers</em></p>

<p>I say mildly interesting, because a lot of this depends on what generated the initial index. It also depends a bit on if and how condition was coded by name, perhaps to keep track of it. If any columns or rows (e.g., dropouts) were removed it’s even harder to trace what happened to get from here to there. Hard to know from where I sit, but it highlights a broader point: Sorting and filter can bring rows closer together, violating the independence assumptions.</p>

<p>Relatedly, consider a hypothetical dataset containing just 2 treatments and outcome variable, sorted by (treatment, outcome). You’ll have the highs from one treatment adjacent in the dataframe with the lows from another. If those cells at the boundary get mangled during data cleaning, all of the mangled cells will correspond with the extrema.</p>

<h2 id="planawalda">Planawalda</h2>

<p>When people are manually editing a dataset, particularly if it’s sorted by a variable of interest, mangling has a better shot at being non-random and adjusting the outcome of a statistical test.</p>

<p>Here’s the rub: Mangling that produces significance will be much more likely to make it into the literature than mangling that erodes an effect. Researchers may be more likely to double check the data if significance is not found. Journals are certainly more likely to publish significant work. Even in a world of randomly mangled data, mangled data would be biased towards significance. We should expect errors in data handling to <em>appear</em> more intentional than expected by chance,if we’re examining the published literature. Strike two against the statistical test.</p>

<p>Finally, and I’ve <a href="http://localhost:4000/blog/2023/pcurve/">written about this before</a>, but applying a statistical test because you’ve noticed a pattern in data is effectively embracing QRPs. At best, there’s a file drawer. How many articles, p-curves, etc., do sleuths find and think something fishy is, only to get non-significant results and drop it? Do we really think the false positive rate is zero?</p>

<p>The point of all of this is that we can’t take the statistical test seriously. The null model is simply incorrect, the observations aren’t independent, and we know there is likely a few layers of selection bias. It certainly doesn’t mean there’s a one in a million chance this happened by chance, and honestly, it might not mean much more than the rows were moved (however, for whatever reason) on a sorted dataset.</p>

<h2 id="the-test-that-never-was">The test that never was</h2>

<p>I’m much more interested in the test that, inexplicably, wasn’t run. Data Colada notes that of the flagged rows, six rows (P#: 7, 12, 51, 52, 92, 101) appear to be out of order. For five of these rows, the movement seems to indicate changes in treatment effect, as the “prior” location was orderly sorted between observations in a different treatment. Although it is buried at the bottom of the blog post, row 92 appears to have been moved but not had its condition changed—chalked up to an accident in the fraud process, incidental, or something weird with CalcChain (we’ll come back to this).</p>

<p>If you think you know what the data looked like before tampering occurred, and you’re alleging that this was done intentionally in service of fraud—wouldn’t it make sense to see if it impacts the results? We can just trivially reassign the treatments as suggested by CalcChain and run a test. If we do, it’s still significant in the same direction (T = -2.1, p = 0.04). The manual alterations to the data were not necessary for the result.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">study_one</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">~/Downloads/S1_flagged_2023_06_09.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="c1">#Change columns as suggested by Data Colada
</span><span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">7</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">12</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">51</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">52</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">101</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1">#Generous here
</span><span class="n">study_one</span><span class="p">.</span><span class="nf">drop_duplicates</span><span class="p">()</span>

<span class="c1">#ttest on condition 1 and 2
</span><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">ttest_ind</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">ttest_ind</span><span class="p">(</span><span class="n">study_one</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">][</span><span class="sh">'</span><span class="s">Deductions</span><span class="sh">'</span><span class="p">],</span> <span class="n">study_one</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">2</span><span class="p">][</span><span class="sh">'</span><span class="s">Deductions</span><span class="sh">'</span><span class="p">]))</span>
</code></pre></div></div>

<h1 id="some-clarity-now">Some Clarity Now!</h1>

<p>Let’s sum things up a little bit.</p>

<ul>
  <li>The data are clearly manually adjusted and rows have been moved.</li>
  <li>The movements increase the effect along the hypothesized direction.</li>
  <li>Unclear how to distinguish this from selection bias, non-independence of rows in a sorted dataset.</li>
  <li>Of the six rows that had their location moved, one did not have its condition changed.</li>
  <li>One row is duplicated.</li>
  <li>The movement of rows does not impact the direction of the effect or significance.</li>
</ul>

<p>There are two scenarios:</p>

<ol>
  <li>
    <p>Data were carelessly mangled in handling.</p>
  </li>
  <li>
    <p>Data were intentionally mangled to support the hypothesis.</p>
  </li>
</ol>

<p>It is abundantly clear that at 1 is true. I agree with Data Colada’s assessment that someone manually interacted with the data and that rows were moved during this process. It’s more than enough to warrant pulling the raw data, digging in, and seeing what went wrong. I hope/assume Harvard has done this.</p>

<p>The bigger question is 2. The condition swaps do align with the hypothesis and are distributed at the extrema of the distributions. You’d expect something like this with fraud. On the other hand, all of the observations (possibly) sort to a small portion of the dataset, and when sorted they’re not really independent. Suggestive perhaps, but not a slam dunk. We also should expect that when mangled data is found in the literature, it will be mangled in a way that biases towards significance.</p>

<p>The fraud thing also requires accepting one of two kinds of weird possibilities. In the cleanest version, the researcher checked for significance, found it, then went back to move a few rows. Perhaps to avoid being p-curved? It’s not clear why they’d go back. They also seem to have moved one row without changing the condition, perhaps as a mistake? Alternatively, you have to assume they did [something else] alongside the moves and decided to combine fraud tactics for [reasons].</p>

<p>Or these rows were clumped together on a sort and clumsily shuffled in around in excel.</p>

<p>When we talk about fraud, it conjures up the classics. Mixed fonts, <a href="https://datacolada.org/98">obvious distributions being added</a> or obvious <a href="http://ecoevoevoeco.blogspot.com/2021/05/">skullduggery in the excel sheet</a>. We don’t see anything in shouting distance of this damning here. Instead, we see rows absolutely moved by hand that could well have been clumped and don’t seem to impact significance. We see the rows do swap conditions in a way we’d expect with fraud, but we’d also expect that after a publication filter and sorting. I wholeheartedly agree this is enough to pull the raw data and trace what happened. However, I don’t have a good enough lawyer to feel comfortable alleging fraud based on the content of the blog post alone.</p>

<h1 id="the-bigger-pictures">The Bigger Picture(s)</h1>

<p>No part of my blog post is intended to serve as a defense of Gino. Harvard has a 1200-page investigation and has put Gino on leave. This broader context cannot be ignored, and it certainly seems indicative of some flavor of misconduct. If I had to make a bet here, I don’t think I’d differ that much from the folks who will be angrily subtweeting me.</p>

<p>The narrower-yet-broader picture regards the blog post in isolation, the evidence it provided, the evidence it didn’t, and the incentives and motivations that fuel it.</p>

<p>I find the use of NHST to establish evidence here peculiar given that the authors  <a href="https://pubmed.ncbi.nlm.nih.gov/22006061/">published the paper</a> on how flexibility in data analysis can allow researchers to present anything as significant. How many papers were examined? How many datasets were pulled? How many authors? Was the pattern noticed before the decision to run the t-test? Is this HARKing?</p>

<p>In the <a href="https://datacolada.org/110">second blog post</a>, why choose a t-test when the variance for one category is zero? What other tests could have been chosen? Aren’t small-N studies unreliable? What’s the power analysis for N=8, and would you run the test if you had conducted a power analysis? None of the assurances and rigor we increasingly expect with published literature can be found here.</p>

<p>It’s also quite telling what tests they didn’t run. Neither in this blog post, <a href="https://datacolada.org/110">nor in the subsequent one</a>, did they check to see if the manipulations they identified impacted the findings. Although they do not explicitly note it, the results they present in Blog Post #2 differ from the original study in that they group two of the conditions. Even dropping the suspicious observations, the test as they implemented it remains significant. It likewise remains significant for the comparison of one of the two conditions when pairwise comparisons are made. There isn’t clear evidence that the suspicious rows here were necessary for significance.</p>

<p>Of course, it isn’t definitive, but it would certainly be a very big piece of the puzzle. If, in both cases, it made or broke significance—that would be telling. It’s very hard to believe that in two years they didn’t think to. Given that in both cases, the effect persists, did they just bury these tests in file drawers?</p>

<p>Similarly, there was not even a whiff of an attempt to consider alternative explanations. It’s obvious that findings from the top of one distribution and the bottom of another can be aligned in a dataset if the dataset is sorted on the observable. There’s also very little effort put into why the observations can clump in a sort—something that doesn’t seem particularly necessary for fraud but is pretty essential for earnest mangling.</p>

<p>Moreover, in discussing what points were swapped around, they bury the fact that one of the data points didn’t change condition at all. This is not something you’d expect with fraudulent data manipulation. It kind of makes sense for mangling. Why dark-pattern this finding away?</p>

<p>In totality, the blog post only presents evidence in favor of these rows being switched due to fraud and either buries or simply fails to mention any competing possibilities. Even if you think they’re far-fetched, they’re plausible, and it doesn’t hurt to bring them up if only to point out why they’re wrong.</p>

<p>DC also uses inappropriate statistical testing to add a bit of quantitative dazzle, without considering if the null model is appropriate or potential sources of bias. Whether or not fraud happened, one thing is clear: The blog posts are trying to convince you that it did and unwilling to even perfunctorily consider anything nuanced or conflicting.</p>

<p>Make no mistake, this is persuasive writing—not a neutral investigation.</p>

<h1 id="clout-and-consequence">Clout and Consequence</h1>

<p>Of course it is.</p>

<p>The authors’ careers over the past decade have been centered around developing and implementing methods to find things “wrong” in the literature. That’s probably how you know them. They have every incentive to find things that are wrong. This is no different from honest researchers who have every incentive to find that the location of a signature implausibly impacts honesty. In both cases, finding what you’re looking for renders clout and prestige. Why else rush to release blog posts after a journalist scoops you, dripping one by one as though it’s some sort of Q-Anon drop?</p>

<p>This is the reason we need to be as critical and skeptical of the critiques as we are of the literature itself. We need to hold both to the same standard of rigor, from pointing out inappropriate use of NHST to noting excluded conflicting evidence and failure to consider alternative explanations. The background and broader context of an investigation don’t matter. We need to have standards.</p>

<p>We need standards because these types of blog posts and Twitter threads have consequences for the mental health and careers of people who are targeted. If we let those standards slide in a context where misconduct seems likely, we can let them slide anywhere. What if you, or your student, years ago sorted and mangled a dataset? Wouldn’t you want to be emailed and given the chance to share the raw data and trace the error before being accused of fraud? Shouldn’t the accusers check if the mangling impacted the results? Should they consider alternative explanations?</p>

<p>Why don’t we have the same rigor for critics and the critiqued? How can we reconcile that widely-held beliefs incentivize rendering most of science incorrect with an assumption that Data Colada and other sleuths bat 1.000 using the same methods? Where are the papers on the false-positive rate for data sleuthing? What blog post shows that someone else’s career-ending blog post was flawed? Why is it okay to use whatever statistical test on N=8, without preregistration or power analysis, on whatever dataset and allege fraud? How big is the file drawer?</p>

<p>The “Data Police” metaphor is imperfect but apt. Police are motivated to find evidence and put bad guys away. It often leads to biased policing, looking where they’ve looked or where their biases compel them. Sometimes, innocent people go to jail. With the law, we try to buffer against this with defense attorneys, formal court proceedings, juries of our peers, presumptions of innocence, and internal affairs to investigate police behaving poorly.</p>

<p>As flawed as our criminal justice system is (boy, is it), it at least has these mechanisms in place. No similarities exist in our policing of the literature. Any critique of Data Colada or similar sleuths leads to a flurry of QTs, sub-tweets, and replies. I fully expect to mute my mentions tomorrow. People expressing discomfort the other day were subjected to racism, misogyny, mockery, accusations of fraud, and endless sealioning. Almost every post noting concerns had a deferential line of roughly “not saying anything bad about the data police, but.” It’s not deference, it’s fear. There’s just no room to consider anything other than the persuasive pieces written by people with an incentive to write them.</p>

<p>Maybe we should look at the bigger picture and ask what the externalities here are, and how they might be mitigated?</p>

<p>Summing up? Sure, I think the data looks fishy, and absolutely, this needs a thorough investigation. I don’t think it needs a motivated blog post trying to convince the reader that doesn’t even pretend to engage with nuance.</p>

<p><sub>coda: I’m going to use the bottom of this post to reserve the right not to respond to angry replies that haven’t read this far and posted a screenshot of the dataset open on their computer. I’m also going to note that I wrote this inside a couple hours, and if you find an issue or mistake… my dms are open.  </sub></p>

<!-- ### A hypothetical 
Branching off into the hypothetical, what if an earlier version of the dataset encoded condition with the easier-to-keep-track-of "Control", "Top", "Bottom". What happens if we add such a column and then sort by it? 

![alt text](/assets/img/sortedyada.png "Title"){: width="750" }
*Table with sorted by named conditions* 

All seven  with evidence of having been moved in the dataset wind up adjacent, with the exception of one unswitched row between them. Note that there is an implicit secondary sort on the row number. It's unclear where the row-numbers derive, but one possibility  -->]]></content><author><name></name></author><category term="statistics" /><category term="stats" /><category term="QMPs" /><category term="critique" /><summary type="html"><![CDATA[Why Sleuthing Needs Standards]]></summary></entry><entry><title type="html">Retire the P-Curve (On Twitter)</title><link href="https://joebakcoleman.com/blog/2023/pcurve/" rel="alternate" type="text/html" title="Retire the P-Curve (On Twitter)" /><published>2023-03-10T13:12:00+00:00</published><updated>2023-03-10T13:12:00+00:00</updated><id>https://joebakcoleman.com/blog/2023/pcurve</id><content type="html" xml:base="https://joebakcoleman.com/blog/2023/pcurve/"><![CDATA[<h1 id="the-p-curves-bold-claims">The P-Curve’s bold claims</h1>
<p>In science, there is a common concern that large swaths of the published literature are simply “wrong”. Although these concerns have been around a long time, they have become a centerpiece of scientific discourse over the past 15 years. Rationales for these concerns are wide ranging, but generally center on the overabundance of statistically significant findings in the literature. Typically, this is chalked up to some combination of the file-drawer problem (most research going unpublished) or Questionable Research Practices—encompassing a wide range of behaviors that might convert a non-significant finding into a significant one.  Scientists find themselves staring at the literature, trying to discern where the baby ends and the bathwater begins. What do we throw out? What do we keep? How can we trust that new results are not P-Hacked?</p>

<p>Enter the P-curve.</p>

<p>Developed in 2013<sup>1</sup>, P-Curves have been the stickiest among a family of meta-analytic tools designed to confront selection bias going back 40 years <sup>2</sup>. Their logic is fairly straightforward, for a given (and fixed) power, the distribution of p-values takes on a characteristic shape. If power is sufficiently high, this distribution will be skewed such that smaller p-values are more common than those close to .05. Without loss of generality, we can calculate a p-curve for the simplest possible case—a one sample Z-Test.</p>

<p><img src="/assets/img/pcurveblog_fig1.png" alt="alt text" title="Title" width="750" />
<em>P-Curves calculated assuming a 1-sample Z-test at various levels of power.</em></p>

<p>Note that as power increases to 90%, the probability of getting a p-value between .01 and .05 drops to less than 20%. For 62% power, the chance of getting a p-value between .025 and .05 is only 8%. Armed with this model, what might we conclude if a study on two effects reports p values of .027 and .039 with 62% average power? The probability of this occurring under the p-curve model is less than 1%.</p>

<h3 id="p-curve-polygraph-and-baton-and-of-the-stats-cop">P-Curve: Polygraph and Baton and of the stats cop</h3>
<p>This feature of p-curves has become the polygraph and baton of self-styled stats cops on Twitter. They argue that an abundance of p-values nearer to .05 cannot be explained by chance and therefore constitutes evidence of p-hacking. Every few months, some study gets a bit of news attention. This corner of Twitter circles the p-values, notes how many are near .05, and then a pile on begins.</p>

<p><img src="/assets/img/Ritchie.png" alt="alt text" title="Title" width="750" />
<em>Informal application of the p-curve to assert p-hacking in a study on cold exposure</em></p>

<p>Of course, there is little doubt that published studies will often require debunking or benefit from critique, and the cold exposure study had its share of issues. However, far too often, this dynamic emerges as a consequence of an early career researcher’s work that found its way into a viral news story. If the researcher followed open-science principles, their data and code are downloaded by dozens of sleuths that effectively reverse p-hack the data into non-significance then hold it up alongside the p-curve as evidence of p-hacking. As you might expect, this does not make open science a <strong><a href="https://www.bps.org.uk/psychologist/bropenscience-broken-science">particularly welcoming place</a></strong>.</p>

<p>What makes these p-curve critiques distinct from something like discussing potential confounds or the appropriateness of statistical tests, is that they carry with them judgement of either the researcher’s abilities or moral standing. They necessarily insinuate that a researcher either does not understand that you can’t engage in QRPs, or has done so knowingly in an attempt to mislead the scientific community for personal gain. These are hefty allegations, and they’re gonna need solid evidence. Does the P-curve give us that?</p>

<h3 id="p-curve-p-hacking-to-detect-p-hacking">P-Curve: P-Hacking to detect P-Hacking</h3>
<p>As initially conceived and often implemented, these p-curve type methods are to be used on chunks of the scientific literature specified <strong>a priori</strong>. This has its own issues and caveats, which are discussed  <strong><a href="https://statmodeling.stat.columbia.edu/2018/02/26/p-curve-p-uniform-hedges-1984-methods-meta-analysis-selection-bias-exchange-blake-mcshane-uri-simosohn/">this excellent post</a></strong>. However, advocates of the p-curve have taken this a step farther, suggesting it should be used to evaluate <strong><a href="http://daniellakens.blogspot.com/2014/05/the-probability-of-p-values-as-function.html">individual studies</a></strong>. Even for studies reporting only a pair of results, journal editors are encouraged to seek explanation from authors with “unlikely” patterns of p-values… or force them to run additional experiments. Lakens <strong><a href="https://daniellakens.blogspot.com/2015/05/after-how-many-p-values-between-0025.html">writes</a></strong>:</p>

<blockquote>
  <p>What should you do as an editor when you encounter a set of studies with p-values that are relatively unlikely to occur? First, you can ask the authors to discuss the situation. For example, when you explicitly mention the set of studies becomes more probable when a non-significant finding is added, the authors might be happy to oblige. Second, one of my favorite solutions is to decide upon an in principle acceptance (assuming the article is fit for publication), but ask the authors to add one replication. The authors are guaranteed of publication irrespective of the outcome of the replication, but we have a better knowledge of what is likely to be true.</p>
</blockquote>

<p>Perhaps this sounds good in principle, but consider it in practice: A twitter data-sleuth sees a news story making the rounds, and decides to examine the paper. They notice the reported significant p-values between .025 and .05, calculate the probability that would have occurred by chance, and find that it is low. Excitedly, they post to Twitter that the surprising, news-worth headline is probably p-hacked. Clout rains down.</p>

<p>Of course, they wouldn’t have calculated this quantity had the p-values been smaller. The decision to run the analysis depended on the data, such that they <em>engaged in p-hacking to detect p-hacking</em>. This could be made worse, as there are plenty of researcher degrees of freedom in estimating probability from a p-curve. At the end of the day, it’s no different than seeing a trend in a scatter plot and finding a regression line yields significance.</p>

<p>Alternatively, the researcher might argue that they apply the test to every paper they encounter. I seriously doubt this is the case for anyone (who has the time?), but this is just data-dredging the published literature for evidence of p-hacking. Unless they tweet every time someone’s paper has plausible p-values, they’re cherry picking the instances in which the test indicates p-hacking. It’s no different than a scientist who claims they can avoid p-hacking by testing every possible relationship and reporting those that are significant.</p>

<p>Worse, they often double down–pulling the data and considering alternative analyses that would have produced non-significance. All of this occurring is conditioned on the data itself, such that the a fair chunk of this sleuthing winds up engaging in the very thing it seeks to eradicate. In practice, applications of the p-curve—particularly on Twitter—are simply <em>p-hacking to prove p-hacking</em>. The shoemaker’s children go shoeless.</p>

<h3 id="a-model-like-any-other">A model like any other</h3>
<p>At it’s core, the p-curve is a model, and inferences drawn from it are only as good as the model assumptions. One of its core assumptions is that the p-values are independent. This is rarely the case within a single paper, as often the reported effects are on the same phenomenon measured in a a similar way from a similar population. This increases the probability that effect and sample sizes are in the same neighborhood, as well as the p-values they generate.</p>

<p>It likewise assumes that all experiments would have been conducted, regardless of the outcomes of of previous experiments. This too is rarely the case, as findings guide additional research. As Richard Morey <strong><a href="https://www.youtube.com/watch?v=c0G98qp1cf4">pointed out</a></strong>, this is a non-trivial distinction with profound consequences for how we interpret findings after the fact.</p>

<p>P-Curves further require that effects are not heterogenous, and that null effects are precisely zero such that they don’t bleed into the p-curve at a rate higher than expected by chance. With discrete outcomes, p-values can only take on finite values, deviating from the smooth p-curve. Violating any of these assumptions, and especially many of them, renders inferences drawn from a p-curve analysis unreliable.</p>

<h3 id="what-p-curves-miss">What P-Curves miss</h3>

<p>Even if a p-curve’s assumptions are met, and the data have plenty of tiny p-values, that provides no evidence the inferences drawn are correct or free from p-hacking. Consider a situation in which a researcher knowingly includes a confound that produces a strong effect. This is a QRP if ever there were one, yet a P-Curve analysis would have us convinced the paper contains evidence with no indication of p-hacking. In a sense it does, but only that the confound is indeed a confound.</p>

<p>This could also happen inadvertently, if a researcher simply selects an inappropriate model for their data. Perhaps the strong effect is simply violating assumptions of equality of variance, or normality. Maybe their model is a bit overfit, or there’s post-treatment bias. Perhaps the model is perfectly fine, but the inferences drawn from it are incorrect given the nature of the data.</p>

<p>It’s unfair to assume p-curves, or any tool, should be able to tell us all of these things. Sorting out if a paper provides useful evidence requires a deep understanding of the inference methods, the data collection process, and the domain knowledge that converts the numbers and maths into something we can understand and care about. But this is precisely the point—we shouldn’t expect to be able to pass any and all papers through a statistical tool and determine much of anything about its value.</p>

<p>By imbuing the p-curve with special and unquestioned power to discern fact from fiction, we’ve created a tool that can be weaponized to push junk science. Consider a recent meta analysis<sup>4</sup> which leveraged the p-curve as evidence of pre-cognition. Just as Twitter sleuths can p-hack the p-curve to dunk on a paper for internet points, people trying to push scientific disinformation can do the same to demonstrate the validity of their claims.</p>

<h1 id="the-take-home">The take-home</h1>
<p>Scientists, pressed for time, too often treat statistical models as oracles that feast on data and yield answers to life’s deepest questions. The reality is that models are quite simple things, which have little to no clue what we want to know and yield an answer regardless. They’re a bit like a less chatty chatGPT. Only if we understand their strengths and limitations can we apply them in a way that will yield useful inference.</p>

<p>Statisticians have yelled themselves hoarse over the paste decade, encouraging scientists to understand what their models can and can’t do, and what their output can and can’t tell us. Scientists, however, have gone a different route—on a crusade against QRPs with much less concern over whether a QRP-free paper yields valid inference. In this quest, the P-Curve has become yet another oracle that we assume can force a distribution of p-values to confess if they were p-hacked.</p>

<p>When a paper differs from the expected distribution, it only tells us that—not why. It could be that the assumptions don’t hold, bad luck on the authors part, null findings stuck in peer review, p-hacking, or just that the rest of the results are somewhere else in the paper. Even when the data match expectations, there’s no guarantee the inferences in the paper are correct. Expecting the p-curve to be the one statistical test that we can uncritically apply to  any dataset is madness. There’s no easy substitute for engaging deeply with a papers methods, data, and conclusions.</p>

<p>The P-curve, for whatever value it may hold, is used on social media in the precisely way we’re all taught not to use statistics. Stats cops read paper after paper, and when the p-values in one look suspicious—they form a hypothesis that it was due to p-hacking. When they run a p-curve, sure enough, the data supports their hypothesis. It’s data dredging, cherry-picking, and HARKing all in one. Worse, their hypothesis leads to the conclusion that they can publicly humiliate the author for their transgression. This isn’t constructive criticism, nor is it effective evaluation of the merits of a paper. It’s just cyberbullying.</p>

<h2 id="references">References</h2>

<ol>
  <li>
    <div class="csl-entry">Simonsohn, U., Nelson, L. D., &#38; Simmons, J. P. (2014). P-curve: a key to the file-drawer. <i>Journal of Experimental Psychology. General</i>, <i>143</i>(2), 534–547. https://doi.org/10.1037/A0033242</div>
  </li>
  <li>
    <div class="csl-entry">Hedges, L. v. (1984). Estimation of Effect Size under Nonrandom Sampling: The Effects of Censoring Studies Yielding Statistically Insignificant Mean Differences. <i>Journal of Educational Statistics</i>, <i>9</i>(1), 61. https://doi.org/10.2307/1164832</div>
  </li>
  <li>
    <div class="csl-entry">Bem, D., Tressoldi, P., Rabeyron, T., &#38; Duggan, M. (2015). Feeling the future: A meta-analysis of 90 experiments on the anomalous anticipation of random future events. <i>F1000Research</i>, <i>4</i>. https://doi.org/10.12688/F1000RESEARCH.7177.2</div>
  </li>
</ol>]]></content><author><name></name></author><category term="statistics" /><category term="stats" /><category term="pcurve" /><category term="twitter" /><summary type="html"><![CDATA[It's past time to stop circling P-values]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://joebakcoleman.com/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog" /><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://joebakcoleman.com/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://joebakcoleman.com/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>