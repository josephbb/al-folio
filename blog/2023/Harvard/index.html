<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Rigor Desiderada | Joe B. Bak-Coleman</title>
    <meta name="author" content="Joe B. Bak-Coleman">
    <meta name="description" content="Why Sleuthing Needs Standards">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://joebakcoleman.com/blog/2023/Harvard/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


<!-- Twitter cards -->
<meta name="twitter:site" content="@jbakcoleman">
<meta name="twitter:creator" content="@">
<meta name="twitter:title" content="Rigor Desiderada">


<meta name="twitter:description" content="Collective behavior from fish to fascists.
">



<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://joebakcoleman.com/assets/img/FishSchool.jpg">

<!-- end of Twitter cards -->

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Joe </span>B. Bak-Coleman</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Rigor Desiderada</h1>
    <p class="post-meta">June 20, 2023</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
        ·  
        <a href="/blog/tag/stats">
          <i class="fas fa-hashtag fa-sm"></i> stats</a>  
          <a href="/blog/tag/qmps">
          <i class="fas fa-hashtag fa-sm"></i> QMPs</a>  
          <a href="/blog/tag/critique">
          <i class="fas fa-hashtag fa-sm"></i> critique</a>  
          
        ·  
        <a href="/blog/category/statistics">
          <i class="fas fa-tag fa-sm"></i> statistics</a>  
          

    </p>
  </header>

  <article class="post-content">
    <h1 id="yada-yada-falsificada">Yada, Yada, Falsificada</h1>

<p>On June 16<sup>th</sup>, Stephanie Lee released <a href="https://www.chronicle.com/article/a-weird-research-misconduct-scandal-about-dishonesty-just-got-weirder" rel="external nofollow noopener" target="_blank">an article</a> in the <em>The Chronicle</em> detailing a new chapter in a long-running scandal over data mishandling in a now-retracted <a href="https://www.pnas.org/doi/10.1073/pnas.1209746109" rel="external nofollow noopener" target="_blank">paper</a> about honesty. We’ve come to learn that a second researcher is being investigated by Harvard, and has currently been <a href="https://www.hbs.edu/faculty/Pages/profile.aspx?facId=271812" rel="external nofollow noopener" target="_blank">placed on leave</a>. While the full report has not been released (and may never be), it looks a lot like Harvard found something pretty damning. If none of this sounds familiar, I suggest reading her article before going much further.</p>

<p>Shortly after Stephanie’s article, <a href="https://datacolada.org/" rel="external nofollow noopener" target="_blank">Data Colada</a> announced a four-part blog series entitled “Data Falsficada” examining fraud on the part of Harvard Business School Professor Francesca Gino. This blog series is ostensibly a teaser of a longer report, compiled starting in 2021 by the Blog’s authors and other “anonymous researchers.”</p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">The first of a four-post series on Data Falsificada<a href="https://t.co/hmKOxlsqBX" rel="external nofollow noopener" target="_blank">https://t.co/hmKOxlsqBX</a></p>— Data Colada (@DataColada) <a href="https://twitter.com/DataColada/status/1670073988201029632?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">June 17, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<h2 id="blog-post-1-oh-sheet">Blog post 1: Oh sheet.</h2>

<p>The <a href="https://datacolada.org/109" rel="external nofollow noopener" target="_blank">first blog post</a> examines <a href="https://www.pnas.org/doi/full/10.1073/pnas.1209746109" rel="external nofollow noopener" target="_blank">already-retracted</a> study, which claimed to show that signing at the top of a document leads to greater dishonesty than signing at the bottom. The key finding can be found in this <em>very chic</em> drop-shadow dynamite plot:</p>

<p><img src="/assets/img/pnas1.png" alt="alt text" title="Title" width="750">
<em>Figure 1 from original manuscript. Caption: “Reported and actual number of math puzzles solved by condition, experiment 1 (n = 101). Error bars represent SEM.”</em></p>

<p>Data Colada’s blog post provides compelling evidence that rows were moved around inside of an excel file. The trick here is that excel files are kinda neat, and don’t both re-organizing the deeper data every time you move something around. Instead, they store a record of what went where. Digging into this record shows that a few of the observations appear to be out of order or duplicated.</p>

<h2 id="fill-em-all-and-let-god-sort-em-out">Fill ‘em all and let god sort ‘em out</h2>

<p>Looking at the swapped around datapoints, a pattern emerges. They’re very neatly aligned with the hypothesized effect, and <em>located on the extrema of each distribution</em>. This second point is absolutely critical. The authors argue that even if the data were inadvertently mishandled, the chance that <em>these</em> observations would have been mishandled and none other is extremely low. They run a statistical test (more on that later) to generate a tiny p-value as quantitative evidence that this pattern could not possibly be expected by chance.</p>

<p>It’s not explicitly stated here, but the null model is that the altered rows are drawn at random from all of the possible rows. To start scratching at whether this is plausible, we need to construct something that looks like the dataset not <em>as is</em> but <em>as was</em>. We can do that with a few lines of python, borrowing from the bottom of their blog post’s estimate of what the conditions likely were.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">7</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">12</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">51</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">52</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">101</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">study_one</span><span class="p">.</span><span class="nf">drop_duplicates</span><span class="p">()</span>
</code></pre></div></div>

<p>Let’s sort these by condition, but lets <em>hypothetically</em> assume that conditions had text labels in a column at an earlier stage—Control, Top, Bottom. Something mildly interesting happens. All of the out-of-sorts rows are next to one another,</p>

<p><img src="/assets/img/sortayada.png" alt="alt text" title="Caption" width="750"></p>

<p><em>Rows sorted by “Name”, with a secondary sort on the original row numbers</em></p>

<p>I say mildly interesting, because a lot of this depends on what generated the initial index. It also depends a bit on if and how condition was coded by name, perhaps to keep track of it. If any columns or rows (e.g., dropouts) were removed it’s even harder to trace what happened to get from here to there. Hard to know from where I sit, but it highlights a broader point: Sorting and filter can bring rows closer together, violating the independence assumptions.</p>

<p>Relatedly, consider a hypothetical dataset containing just 2 treatments and outcome variable, sorted by (treatment, outcome). You’ll have the highs from one treatment adjacent in the dataframe with the lows from another. If those cells at the boundary get mangled during data cleaning, all of the mangled cells will correspond with the extrema.</p>

<h2 id="planawalda">Planawalda</h2>

<p>When people are manually editing a dataset, particularly if it’s sorted by a variable of interest, mangling has a better shot at being non-random and adjusting the outcome of a statistical test.</p>

<p>Here’s the rub: Mangling that produces significance will be much more likely to make it into the literature than mangling that erodes an effect. Researchers may be more likely to double check the data if significance is not found. Journals are certainly more likely to publish significant work. Even in a world of randomly mangled data, mangled data would be biased towards significance. We should expect errors in data handling to <em>appear</em> more intentional than expected by chance,if we’re examining the published literature. Strike two against the statistical test.</p>

<p>Finally, and I’ve <a href="http://localhost:4000/blog/2023/pcurve/" rel="external nofollow noopener" target="_blank">written about this before</a>, but applying a statistical test because you’ve noticed a pattern in data is effectively embracing QRPs. At best, there’s a file drawer. How many articles, p-curves, etc., do sleuths find and think something fishy is, only to get non-significant results and drop it? Do we really think the false positive rate is zero?</p>

<p>The point of all of this is that we can’t take the statistical test seriously. The null model is simply incorrect, the observations aren’t independent, and we know there is likely a few layers of selection bias. It certainly doesn’t mean there’s a one in a million chance this happened by chance, and honestly, it might not mean much more than the rows were moved (however, for whatever reason) on a sorted dataset.</p>

<h2 id="the-test-that-never-was">The test that never was</h2>

<p>I’m much more interested in the test that, inexplicably, wasn’t run. Data Colada notes that of the flagged rows, six rows (P#: 7, 12, 51, 52, 92, 101) appear to be out of order. For five of these rows, the movement seems to indicate changes in treatment effect, as the “prior” location was orderly sorted between observations in a different treatment. Although it is buried at the bottom of the blog post, row 92 appears to have been moved but not had its condition changed—chalked up to an accident in the fraud process, incidental, or something weird with CalcChain (we’ll come back to this).</p>

<p>If you think you know what the data looked like before tampering occurred, and you’re alleging that this was done intentionally in service of fraud—wouldn’t it make sense to see if it impacts the results? We can just trivially reassign the treatments as suggested by CalcChain and run a test. If we do, it’s still significant in the same direction (T = -2.1, p = 0.04). The manual alterations to the data were not necessary for the result.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">study_one</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">~/Downloads/S1_flagged_2023_06_09.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="c1">#Change columns as suggested by Data Colada
</span><span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">7</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">12</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">51</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">52</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">study_one</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">P#</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">101</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1">#Generous here
</span><span class="n">study_one</span><span class="p">.</span><span class="nf">drop_duplicates</span><span class="p">()</span>

<span class="c1">#ttest on condition 1 and 2
</span><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">ttest_ind</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">ttest_ind</span><span class="p">(</span><span class="n">study_one</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">][</span><span class="sh">'</span><span class="s">Deductions</span><span class="sh">'</span><span class="p">],</span> <span class="n">study_one</span><span class="p">[</span><span class="n">study_one</span><span class="p">[</span><span class="sh">'</span><span class="s">Cond</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">2</span><span class="p">][</span><span class="sh">'</span><span class="s">Deductions</span><span class="sh">'</span><span class="p">]))</span>
</code></pre></div></div>

<h1 id="some-clarity-now">Some Clarity Now!</h1>

<p>Let’s sum things up a little bit.</p>

<ul>
  <li>The data are clearly manually adjusted and rows have been moved.</li>
  <li>The movements increase the effect along the hypothesized direction.</li>
  <li>Unclear how to distinguish this from selection bias, non-independence of rows in a sorted dataset.</li>
  <li>Of the six rows that had their location moved, one did not have its condition changed.</li>
  <li>One row is duplicated.</li>
  <li>The movement of rows does not impact the direction of the effect or significance.</li>
</ul>

<p>There are two scenarios:</p>

<ol>
  <li>
    <p>Data were carelessly mangled in handling.</p>
  </li>
  <li>
    <p>Data were intentionally mangled to support the hypothesis.</p>
  </li>
</ol>

<p>It is abundantly clear that at 1 is true. I agree with Data Colada’s assessment that someone manually interacted with the data and that rows were moved during this process. It’s more than enough to warrant pulling the raw data, digging in, and seeing what went wrong. I hope/assume Harvard has done this.</p>

<p>The bigger question is 2. The condition swaps do align with the hypothesis and are distributed at the extrema of the distributions. You’d expect something like this with fraud. On the other hand, all of the observations (possibly) sort to a small portion of the dataset, and when sorted they’re not really independent. Suggestive perhaps, but not a slam dunk. We also should expect that when mangled data is found in the literature, it will be mangled in a way that biases towards significance.</p>

<p>The fraud thing also requires accepting one of two kinds of weird possibilities. In the cleanest version, the researcher checked for significance, found it, then went back to move a few rows. Perhaps to avoid being p-curved? It’s not clear why they’d go back. They also seem to have moved one row without changing the condition, perhaps as a mistake? Alternatively, you have to assume they did [something else] alongside the moves and decided to combine fraud tactics for [reasons].</p>

<p>Or these rows were clumped together on a sort and clumsily shuffled in around in excel.</p>

<p>When we talk about fraud, it conjures up the classics. Mixed fonts, <a href="https://datacolada.org/98" rel="external nofollow noopener" target="_blank">obvious distributions being added</a> or obvious <a href="http://ecoevoevoeco.blogspot.com/2021/05/" rel="external nofollow noopener" target="_blank">skullduggery in the excel sheet</a>. We don’t see anything in shouting distance of this damning here. Instead, we see rows absolutely moved by hand that could well have been clumped and don’t seem to impact significance. We see the rows do swap conditions in a way we’d expect with fraud, but we’d also expect that after a publication filter and sorting. I wholeheartedly agree this is enough to pull the raw data and trace what happened. However, I don’t have a good enough lawyer to feel comfortable alleging fraud based on the content of the blog post alone.</p>

<h1 id="the-bigger-pictures">The Bigger Picture(s)</h1>

<p>No part of my blog post is intended to serve as a defense of Gino. Harvard has a 1200-page investigation and has put Gino on leave. This broader context cannot be ignored, and it certainly seems indicative of some flavor of misconduct. If I had to make a bet here, I don’t think I’d differ that much from the folks who will be angrily subtweeting me.</p>

<p>The narrower-yet-broader picture regards the blog post in isolation, the evidence it provided, the evidence it didn’t, and the incentives and motivations that fuel it.</p>

<p>I find the use of NHST to establish evidence here peculiar given that the authors  <a href="https://pubmed.ncbi.nlm.nih.gov/22006061/" rel="external nofollow noopener" target="_blank">published the paper</a> on how flexibility in data analysis can allow researchers to present anything as significant. How many papers were examined? How many datasets were pulled? How many authors? Was the pattern noticed before the decision to run the t-test? Is this HARKing?</p>

<p>In the <a href="https://datacolada.org/110" rel="external nofollow noopener" target="_blank">second blog post</a>, why choose a t-test when the variance for one category is zero? What other tests could have been chosen? Aren’t small-N studies unreliable? What’s the power analysis for N=8, and would you run the test if you had conducted a power analysis? None of the assurances and rigor we increasingly expect with published literature can be found here.</p>

<p>It’s also quite telling what tests they didn’t run. Neither in this blog post, <a href="https://datacolada.org/110" rel="external nofollow noopener" target="_blank">nor in the subsequent one</a>, did they check to see if the manipulations they identified impacted the findings. Although they do not explicitly note it, the results they present in Blog Post #2 differ from the original study in that they group two of the conditions. Even dropping the suspicious observations, the test as they implemented it remains significant. It likewise remains significant for the comparison of one of the two conditions when pairwise comparisons are made. There isn’t clear evidence that the suspicious rows here were necessary for significance.</p>

<p>Of course, it isn’t definitive, but it would certainly be a very big piece of the puzzle. If, in both cases, it made or broke significance—that would be telling. It’s very hard to believe that in two years they didn’t think to. Given that in both cases, the effect persists, did they just bury these tests in file drawers?</p>

<p>Similarly, there was not even a whiff of an attempt to consider alternative explanations. It’s obvious that findings from the top of one distribution and the bottom of another can be aligned in a dataset if the dataset is sorted on the observable. There’s also very little effort put into why the observations can clump in a sort—something that doesn’t seem particularly necessary for fraud but is pretty essential for earnest mangling.</p>

<p>Moreover, in discussing what points were swapped around, they bury the fact that one of the data points didn’t change condition at all. This is not something you’d expect with fraudulent data manipulation. It kind of makes sense for mangling. Why dark-pattern this finding away?</p>

<p>In totality, the blog post only presents evidence in favor of these rows being switched due to fraud and either buries or simply fails to mention any competing possibilities. Even if you think they’re far-fetched, they’re plausible, and it doesn’t hurt to bring them up if only to point out why they’re wrong.</p>

<p>DC also uses inappropriate statistical testing to add a bit of quantitative dazzle, without considering if the null model is appropriate or potential sources of bias. Whether or not fraud happened, one thing is clear: The blog posts are trying to convince you that it did and unwilling to even perfunctorily consider anything nuanced or conflicting.</p>

<p>Make no mistake, this is persuasive writing—not a neutral investigation.</p>

<h1 id="clout-and-consequence">Clout and Consequence</h1>

<p>Of course it is.</p>

<p>The authors’ careers over the past decade have been centered around developing and implementing methods to find things “wrong” in the literature. That’s probably how you know them. They have every incentive to find things that are wrong. This is no different from honest researchers who have every incentive to find that the location of a signature implausibly impacts honesty. In both cases, finding what you’re looking for renders clout and prestige. Why else rush to release blog posts after a journalist scoops you, dripping one by one as though it’s some sort of Q-Anon drop?</p>

<p>This is the reason we need to be as critical and skeptical of the critiques as we are of the literature itself. We need to hold both to the same standard of rigor, from pointing out inappropriate use of NHST to noting excluded conflicting evidence and failure to consider alternative explanations. The background and broader context of an investigation don’t matter. We need to have standards.</p>

<p>We need standards because these types of blog posts and Twitter threads have consequences for the mental health and careers of people who are targeted. If we let those standards slide in a context where misconduct seems likely, we can let them slide anywhere. What if you, or your student, years ago sorted and mangled a dataset? Wouldn’t you want to be emailed and given the chance to share the raw data and trace the error before being accused of fraud? Shouldn’t the accusers check if the mangling impacted the results? Should they consider alternative explanations?</p>

<p>Why don’t we have the same rigor for critics and the critiqued? How can we reconcile that widely-held beliefs incentivize rendering most of science incorrect with an assumption that Data Colada and other sleuths bat 1.000 using the same methods? Where are the papers on the false-positive rate for data sleuthing? What blog post shows that someone else’s career-ending blog post was flawed? Why is it okay to use whatever statistical test on N=8, without preregistration or power analysis, on whatever dataset and allege fraud? How big is the file drawer?</p>

<p>The “Data Police” metaphor is imperfect but apt. Police are motivated to find evidence and put bad guys away. It often leads to biased policing, looking where they’ve looked or where their biases compel them. Sometimes, innocent people go to jail. With the law, we try to buffer against this with defense attorneys, formal court proceedings, juries of our peers, presumptions of innocence, and internal affairs to investigate police behaving poorly.</p>

<p>As flawed as our criminal justice system is (boy, is it), it at least has these mechanisms in place. No similarities exist in our policing of the literature. Any critique of Data Colada or similar sleuths leads to a flurry of QTs, sub-tweets, and replies. I fully expect to mute my mentions tomorrow. People expressing discomfort the other day were subjected to racism, misogyny, mockery, accusations of fraud, and endless sealioning. Almost every post noting concerns had a deferential line of roughly “not saying anything bad about the data police, but.” It’s not deference, it’s fear. There’s just no room to consider anything other than the persuasive pieces written by people with an incentive to write them.</p>

<p>Maybe we should look at the bigger picture and ask what the externalities here are, and how they might be mitigated?</p>

<p>Summing up? Sure, I think the data looks fishy, and absolutely, this needs a thorough investigation. I don’t think it needs a motivated blog post trying to convince the reader that doesn’t even pretend to engage with nuance.</p>

<p><sub>coda: I’m going to use the bottom of this post to reserve the right not to respond to angry replies that haven’t read this far and posted a screenshot of the dataset open on their computer. I’m also going to note that I wrote this inside a couple hours, and if you find an issue or mistake… my dms are open.  </sub></p>

<!-- ### A hypothetical 
Branching off into the hypothetical, what if an earlier version of the dataset encoded condition with the easier-to-keep-track-of "Control", "Top", "Bottom". What happens if we add such a column and then sort by it? 

![alt text](/assets/img/sortedyada.png "Title"){: width="750" }
*Table with sorted by named conditions* 

All seven  with evidence of having been moved in the dataset wind up adjacent, with the exception of one unswitched row between them. Note that there is an implicit secondary sort on the row number. It's unclear where the row-numbers derive, but one possibility  -->

  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/replication/">Replication failures from fishy science</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/pcurve/">Retire the P-Curve (On Twitter)</a>
  </li>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Joe B. Bak-Coleman. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
